# ğŸ¤ Reconnaissance des Ã‰motions par la Voix

Un systÃ¨me de reconnaissance d'Ã©motions basÃ© sur l'analyse de la voix utilisant l'apprentissage profond et l'API FastAPI avec interface Streamlit.

## ğŸš€ FonctionnalitÃ©s

- **Analyse audio** : Extraction de caractÃ©ristiques MFCC
- **ModÃ¨le CNN-BiLSTM** : Architecture hybride pour la classification d'Ã©motions
- **API REST** : Interface FastAPI pour les prÃ©dictions
- **Interface web** : Application Streamlit pour l'upload et la visualisation
- **Support EMO-DB** : Compatible avec le corpus Berlin Database of Emotional Speech

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- pip
- Git

## ğŸ› ï¸ Installation

### 1. Cloner le repository

```bash
git clone https://github.com/votre-username/emotion-voice-recognition.git
cd emotion-voice-recognition
```

### 2. CrÃ©er un environnement virtuel

```bash
python -m venv venv
```

### 3. Activer l'environnement virtuel

**Windows :**

```bash
venv\Scripts\activate
```

**Linux/Mac :**

```bash
source venv/bin/activate
```

### 4. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

## ğŸ¯ Utilisation

### 1. EntraÃ®ner le modÃ¨le

```bash
python train.py
```

### 2. Lancer l'API FastAPI

```bash
uvicorn main_api:app --reload
```

L'API sera accessible sur : http://localhost:8000

### 3. Lancer l'interface Streamlit

```bash
streamlit run app.py
```

L'interface sera accessible sur : http://localhost:8501

## ğŸ“ Structure du projet

```
â”œâ”€â”€ app.py                 # Interface Streamlit
â”œâ”€â”€ main_api.py           # API FastAPI
â”œâ”€â”€ train.py              # Script d'entraÃ®nement
â”œâ”€â”€ requirements.txt      # DÃ©pendances Python
â”œâ”€â”€ README.md            # Documentation
â”œâ”€â”€ .gitignore           # Fichiers Ã  ignorer
â”œâ”€â”€ data/
â”‚   â””â”€â”€ emodb/           # DonnÃ©es audio EMO-DB
â”œâ”€â”€ features/
â”‚   â””â”€â”€ extract_features.py  # Extraction des caractÃ©ristiques
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_cnn_bilstm.py  # Architecture du modÃ¨le
â”‚   â”œâ”€â”€ saved_model.h5       # ModÃ¨le entraÃ®nÃ©
â”‚   â””â”€â”€ labels.json          # Mapping des labels
â”œâ”€â”€ predictions/
â”‚   â””â”€â”€ predict.py           # Fonction de prÃ©diction
â””â”€â”€ utils/
    â”œâ”€â”€ audio_utils.py       # Utilitaires audio
    â””â”€â”€ data_loader.py       # Chargement des donnÃ©es
```

## ğŸµ Format des donnÃ©es

Le systÃ¨me utilise le corpus EMO-DB avec le mapping suivant :

- **W** = Wut (colÃ¨re)
- **L** = Langeweile (ennui)
- **E** = Ekel (dÃ©goÃ»t)
- **A** = Angst (peur)
- **F** = Freude (joie)
- **T** = Traurigkeit (tristesse)
- **N** = Neutral (neutre)

## ğŸ”§ API Endpoints

### POST /predict

Upload un fichier audio (.wav) pour obtenir la prÃ©diction d'Ã©motion.

**Exemple de rÃ©ponse :**

```json
{
  "prediction": 3,
  "label": "joie"
}
```

## ğŸ“Š Performance

- **Accuracy d'entraÃ®nement** : ~78%
- **Accuracy de validation** : ~66%
- **Architecture** : CNN + BiLSTM
- **Features** : MFCC (40 coefficients)

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¨â€ğŸ’» Auteur

**Votre Nom** - [votre-email@example.com](mailto:votre-email@example.com)

## ğŸ™ Remerciements

- [EMO-DB](http://emodb.bilderbar.info/) pour le corpus de donnÃ©es
- [Librosa](https://librosa.org/) pour l'analyse audio
- [TensorFlow](https://tensorflow.org/) pour l'apprentissage profond
- [FastAPI](https://fastapi.tiangolo.com/) pour l'API
- [Streamlit](https://streamlit.io/) pour l'interface
